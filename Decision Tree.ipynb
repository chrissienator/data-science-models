{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43efe489",
   "metadata": {},
   "source": [
    "# Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295197a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Decision Tree is a non-parametric supervised learning method. It is a model that predicts\n",
    "    the value of a target variable by learning simple decision rules inferred from the features. \n",
    "    It can be seen as a piecewise constant approximation.\n",
    "\n",
    "    Needed packages:\n",
    "    import numpy as np\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fit_intercept : bool, default = True,\n",
    "        Specifies if a constant should be added to the regression.\n",
    "    \n",
    "    batch_size : int, default = 1000\n",
    "        Size of the batch in SGD loop.\n",
    "    \n",
    "    no_epochs : int, default = 10000\n",
    "        Number of epochs taken for the SGD to converge.\n",
    "        \n",
    "    learning_rate : float, default = 0.01\n",
    "        Learning rate in SGD.\n",
    "    \n",
    "    tolerance : float, default = 1e-10\n",
    "        Tolerance for stopping criteria.\n",
    "    \n",
    "    verbose : bool, default = False\n",
    "        Boolean to print or suppress epochs in SGD loop.\n",
    "        \n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    n_features : int\n",
    "        Number of features seen during method `fit`, excluding intercept.\n",
    "        \n",
    "    classes_ : ndarray of shape n_classes\n",
    "        A list of class labels seen during method `fit`.\n",
    "    \n",
    "    n_classes : int\n",
    "        Number of target classes seen during method `fit`.\n",
    "\n",
    "    \n",
    "    Methods\n",
    "    ----------\n",
    "    fit(X,y):\n",
    "        Fit Logistic regression model.\n",
    "        \n",
    "    predict(X, boundary=0.5):\n",
    "        Predict using the fitted parameters of this Logistic Regression estimator.\n",
    "        \n",
    "    predict_prob(X, boundary=0.5):\n",
    "        Predict probability using the fitted parameters of this Logistic Regression estimator.\n",
    "        \n",
    "    accuracy(y, y_pred)\n",
    "        Print accuracy of fitted Logistic Regression estimator\n",
    "    \n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[-1, 1], [1, 2], [3, 4], [-3, 1], [-1, 0]])\n",
    "    >>> y = np.array([1, 0, 1, 0, 0])\n",
    "\n",
    "    >>> regfit = LogisticRegression(verbose=False, no_epochs=100000, tolerance=1e-10, fit_intercept=True).fit(X,y)\n",
    "    >>> regfit.n_features\n",
    "    2\n",
    "    >>> regfit.classes\n",
    "    array([0, 1])\n",
    "    >>> regfit.n_classes\n",
    "    2\n",
    "    >>> regfit.coefficients\n",
    "    array([[-1.93422523],\n",
    "           [ 0.04175192],\n",
    "           [ 0.93863707]])\n",
    "    >>> regfit.intercept\n",
    "    array([-1.93422523])\n",
    "    >>> regfit.residuals\n",
    "    array([[-0.73833641],\n",
    "           [ 0.49620032],\n",
    "           [-0.12503628],\n",
    "           [ 0.24585473],\n",
    "           [ 0.1217484 ]])\n",
    "    >>> regfit.deviance_residuals\n",
    "    array([[-1.63749542],\n",
    "           [ 1.17096248],\n",
    "           [-0.5168614 ],\n",
    "           [ 0.75122601],\n",
    "           [ 0.50955307]])\n",
    "    >>> regfit.predict(X)\n",
    "    array([0, 0, 1, 0, 0])\n",
    "    >>> regfit.predict_prob(X)\n",
    "    array([[0.73833645, 0.26166355],\n",
    "           [0.50379972, 0.49620028],\n",
    "           [0.12503625, 0.87496375],\n",
    "           [0.75414522, 0.24585478],\n",
    "           [0.87825167, 0.12174833]])\n",
    "    >>> regfit.accuracy(y, regfit.y_pred)\n",
    "    0.8\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        fit_intercept = True,\n",
    "        batch_size = 1000, \n",
    "        no_epochs = 10000, \n",
    "        learning_rate = 0.01,\n",
    "        tolerance = 1e-10,\n",
    "        verbose = False,\n",
    "    ):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.batch_size = batch_size\n",
    "        self.no_epochs = no_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    @staticmethod\n",
    "    def gini_impurity(z):\n",
    "        p = np.unique(z, return_counts=True)[1]/y.shape[0]\n",
    "        gini = 1 - np.sum(p**2)\n",
    "        return gini\n",
    "    \n",
    "    @staticmethod\n",
    "    def entropy(z):\n",
    "        a = np.unique(z, return_counts=True)[1]/z.shape[0]\n",
    "        entropy = np.sum(-a * np.log2(a + 1e-99))\n",
    "        return entropy\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit Logistic regression model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape Nxk (Training data)\n",
    "        y : array of shape N (Target values)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted Estimator.\n",
    "        \"\"\"\n",
    "        # Define number of features and classes\n",
    "        self.n_features = len(X[0])\n",
    "        self.classes = np.unique(y)\n",
    "        self.n_classes = len(self.classes)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "       \n",
    "    def predict(self, X, boundary=0.5):\n",
    "        \"\"\"\n",
    "        Predict target using the fitted parameters of this Logistic Regression estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape M, k-1\n",
    "            Values for regressors test data. (Don't add 1 for the intercept)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array of shape M\n",
    "            Returns predicted values.\n",
    "        \"\"\"\n",
    "        \n",
    "        if hasattr(self, 'coefficients') == False:\n",
    "                raise ValueError(\n",
    "                     \" This LogisticRegression instance is not fitted yet.\" \\\n",
    "                     \" Call 'fit' method with appropriate X and y before using this predict function.\"\n",
    "                 )\n",
    "        \n",
    "        \n",
    "        return self.y_pred\n",
    "    \n",
    "    def predict_prob(self, X, boundary=0.5):\n",
    "        \"\"\"\n",
    "        Predict probability using the fitted parameters of this Logistic Regression estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape M, k-1\n",
    "            Values for regressors test data. (Don't add 1 for the intercept)\n",
    "            \n",
    "        boundary : float\n",
    "            Value of decision boundary. If estimated probability > boundary, predict 1.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array of shape M\n",
    "            Returns predicted values.\n",
    "        \"\"\"\n",
    "        \n",
    "        if hasattr(self, 'coefficients') == False:\n",
    "                raise ValueError(\n",
    "                     \" This LogisticRegression instance is not fitted yet.\" \\\n",
    "                     \" Call 'fit' method with appropriate X and y before using this predict function.\"\n",
    "                 )\n",
    "        \n",
    "        self.predict(X)\n",
    "        \n",
    "        probs = np.append(1-self.y_hat, self.y_hat, axis=1)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def accuracy(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Print accuracy of fitted Decision Tree estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array of shape N\n",
    "            Values for true test data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : constant\n",
    "            Returns accuracy.\n",
    "        \"\"\"\n",
    "        \n",
    "        if hasattr(self, 'coefficients') == False:\n",
    "                raise ValueError(\n",
    "                     \" This LogisticRegression instance is not fitted yet.\" \\\n",
    "                     \" Call 'fit' method with appropriate X and y before using this predict function.\"\n",
    "                 )\n",
    "                \n",
    "        ## ...\n",
    "        ## ...\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3e035",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "X = np.array([[-1, 1], [1, 2], [3, 4], [-3, 1], [-1, 0]])\n",
    "y = np.array([1, 0, 1, 0, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed9fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30df7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42347a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef79de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a4b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(z):\n",
    "    p = np.unique(z, return_counts=True)[1]/z.shape[0]\n",
    "    gini = 1 - np.sum(p**2)\n",
    "    return gini\n",
    "\n",
    "def entropy(z):\n",
    "    a = np.unique(z, return_counts=True)[1]/z.shape[0]\n",
    "    entropy = np.sum(-a * np.log2(a + 1e-99))\n",
    "    return entropy\n",
    "\n",
    "def information_gain(y, split, func=entropy):\n",
    "    '''\n",
    "    Returns the Information Gain of a variable given a loss function.\n",
    "    y : \n",
    "        Target variable.\n",
    "    split: \n",
    "        Split choice.\n",
    "    func: \n",
    "        Function to be used to calculate Information Gain in case os classification.\n",
    "    '''\n",
    "    a = sum(split)\n",
    "    b = split.shape[0] - a\n",
    "    \n",
    "    if(a == 0 or b ==0): \n",
    "        ig = 0\n",
    "\n",
    "    else:\n",
    "        if y.dtypes != 'O':\n",
    "            ig = y.var() - (a/(a+b)* y[split].var()) - (b/(a+b)*y[-split].var())\n",
    "        else:\n",
    "            ig = func(y)-a/(a+b)*func(y[split])-b/(a+b)*func(y[-split])\n",
    "\n",
    "    return ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d30b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [['Male',174,96,4], \n",
    "          ['Male',189,87,2], \n",
    "          ['Female',185,110,4],\n",
    "          ['Female',195,104,3],\n",
    "          ['Male',149,61,3]]\n",
    "\n",
    "y = pd.DataFrame(values, columns = ['Gender' , 'Height' , 'Weight'  ,'Index'])\n",
    "\n",
    "y['obese'] = (y.Index >= 4).astype('int')\n",
    "y.drop('Index', axis = 1, inplace = True)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a26bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain(y['obese'], y['Gender'] == 'Male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c8187a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e8224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029837e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1ec7a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Misclassified when cutting at 100kg: 18 \n",
      " Misclassified when cutting at 80kg: 63\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"500data2.csv\")\n",
    "data['obese'] = (data.Index >= 4).astype('int')\n",
    "data.drop('Index', axis = 1, inplace = True)\n",
    "\n",
    "print(\n",
    "  \" Misclassified when cutting at 100kg:\",\n",
    "  data.loc[(data['Weight']>=100) & (data['obese']==0),:].shape[0], \"\\n\",\n",
    "  \"Misclassified when cutting at 80kg:\",\n",
    "  data.loc[(data['Weight']>=80) & (data['obese']==0),:].shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c5a5a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4998"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gini_impurity(y):\n",
    "  '''\n",
    "  Given a Pandas Series, it calculates the Gini Impurity. \n",
    "  y: variable with which calculate Gini Impurity.\n",
    "  '''\n",
    "  if isinstance(y, pd.Series):\n",
    "    p = y.value_counts()/y.shape[0]\n",
    "    gini = 1-np.sum(p**2)\n",
    "    return(gini)\n",
    "\n",
    "  else:\n",
    "    raise('Object must be a Pandas Series.')\n",
    "\n",
    "gini_impurity(data.Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2117324d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997114388674198"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entropy(y):\n",
    "  '''\n",
    "  Given a Pandas Series, it calculates the entropy. \n",
    "  y: variable with which calculate entropy.\n",
    "  '''\n",
    "  if isinstance(y, pd.Series):\n",
    "    a = y.value_counts()/y.shape[0]\n",
    "    entropy = np.sum(-a*np.log2(a+1e-9))\n",
    "    return(entropy)\n",
    "\n",
    "  else:\n",
    "    raise('Object must be a Pandas Series.')\n",
    "\n",
    "entropy(data.Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee18b8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0002808244603327431"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def variance(y):\n",
    "  '''\n",
    "  Function to help calculate the variance avoiding nan.\n",
    "  y: variable to calculate variance to. It should be a Pandas Series.\n",
    "  '''\n",
    "  if(len(y) == 1):\n",
    "    return 0\n",
    "  else:\n",
    "    return y.var()\n",
    "\n",
    "def information_gain(y, mask, func=entropy):\n",
    "    '''\n",
    "      It returns the Information Gain of a variable given a loss function.\n",
    "      y: target variable.\n",
    "      mask: split choice.\n",
    "      func: function to be used to calculate Information Gain in case os classification.\n",
    "    '''\n",
    "    a = sum(mask)\n",
    "    b = mask.shape[0] - a\n",
    "    \n",
    "    if(a == 0 or b ==0): ig = 0\n",
    "    else:\n",
    "        if y.dtypes != 'O':\n",
    "            ig = variance(y) - (a/(a+b)* variance(y[mask])) - (b/(a+b)*variance(y[-mask]))\n",
    "        else:\n",
    "            ig = func(y)-a/(a+b)*func(y[mask])-b/(a+b)*func(y[-mask])\n",
    "    \n",
    "    return ig\n",
    "\n",
    "information_gain(data['obese'], data['Gender'] == 'Male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8a11c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best split for Weight is when the variable is less than  103 \n",
      "Information Gain for that split is: 0.10625190497954848\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def categorical_options(a):\n",
    "    '''\n",
    "    Creates all possible combinations from a Pandas Series.\n",
    "    a: Pandas Series from where to get all possible combinations. \n",
    "    '''\n",
    "    a = a.unique()\n",
    "\n",
    "    opciones = []\n",
    "    \n",
    "    for L in range(0, len(a)+1):\n",
    "        for subset in itertools.combinations(a, L):\n",
    "            subset = list(subset)\n",
    "            opciones.append(subset)\n",
    "\n",
    "    return opciones[1:-1]\n",
    "\n",
    "def max_information_gain_split(x, y, func=entropy):\n",
    "    '''\n",
    "    Given a predictor & target variable, returns the best split, the error and the type of variable based on a selected cost function.\n",
    "    x: predictor variable as Pandas Series.\n",
    "    y: target variable as Pandas Series.\n",
    "    func: function to be used to calculate the best split.\n",
    "    '''\n",
    "\n",
    "    split_value = []\n",
    "    ig = [] \n",
    "\n",
    "    numeric_variable = True if x.dtypes != 'O' else False\n",
    "\n",
    "    # Create options according to variable type\n",
    "    if numeric_variable:\n",
    "        options = x.sort_values().unique()[1:]\n",
    "    else: \n",
    "        options = categorical_options(x)\n",
    "\n",
    "    # Calculate ig for all values\n",
    "    for val in options:\n",
    "        mask =   x < val if numeric_variable else x.isin(val)\n",
    "        val_ig = information_gain(y, mask, func)\n",
    "        # Append results\n",
    "        ig.append(val_ig)\n",
    "        split_value.append(val)\n",
    "\n",
    "    # Check if there are more than 1 results if not, return False\n",
    "    if len(ig) == 0:\n",
    "        return(None,None,None, False)\n",
    "\n",
    "    else:\n",
    "        # Get results with highest IG\n",
    "        best_ig = max(ig)\n",
    "        best_ig_index = ig.index(best_ig)\n",
    "        best_split = split_value[best_ig_index]\n",
    "        \n",
    "        return(best_ig,best_split,numeric_variable, True)\n",
    "\n",
    "\n",
    "weight_ig, weight_slpit, _, _ = max_information_gain_split(data['Weight'], data['obese'],)  \n",
    "\n",
    "\n",
    "print(\n",
    "  \"The best split for Weight is when the variable is less than \",\n",
    "  weight_slpit,\"\\nInformation Gain for that split is:\", weight_ig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccf645ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000281</td>\n",
       "      <td>0.019684</td>\n",
       "      <td>0.106252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Male]</td>\n",
       "      <td>174</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gender    Height    Weight\n",
       "0 -0.000281  0.019684  0.106252\n",
       "1    [Male]       174       103\n",
       "2     False      True      True\n",
       "3      True      True      True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop('obese', axis= 1).apply(max_information_gain_split, y = data['obese'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93bc0a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split(y, data):\n",
    "    '''\n",
    "    Given a data, select the best split and return the variable, the value, the variable type and the IG.\n",
    "    y: name of the target variable\n",
    "    data: dataframe where to find the best split.\n",
    "    '''\n",
    "    masks = data.drop(y, axis= 1).apply(max_information_gain_split, y = data[y])\n",
    "    if sum(masks.loc[3,:]) == 0:\n",
    "        return(None, None, None, None)\n",
    "\n",
    "    else:\n",
    "        # Get only masks that can be splitted\n",
    "        masks = masks.loc[:,masks.loc[3,:]]\n",
    "\n",
    "        # Get the results for split with highest IG\n",
    "        split_variable = max(masks)\n",
    "        #split_valid = masks[split_variable][]\n",
    "        split_value = masks[split_variable][1] \n",
    "        split_ig = masks[split_variable][0]\n",
    "        split_numeric = masks[split_variable][2]\n",
    "\n",
    "        return(split_variable, split_value, split_ig, split_numeric)\n",
    "\n",
    "\n",
    "def make_split(variable, value, data, is_numeric):\n",
    "    '''\n",
    "    Given a data and a split conditions, do the split.\n",
    "    variable: variable with which make the split.\n",
    "    value: value of the variable to make the split.\n",
    "    data: data to be splitted.\n",
    "    is_numeric: boolean considering if the variable to be splitted is numeric or not.\n",
    "    '''\n",
    "    if is_numeric:\n",
    "        data_1 = data[data[variable] < value]\n",
    "        data_2 = data[(data[variable] < value) == False]\n",
    "\n",
    "    else:\n",
    "        data_1 = data[data[variable].isin(value)]\n",
    "        data_2 = data[(data[variable].isin(value)) == False]\n",
    "\n",
    "    return(data_1,data_2)\n",
    "\n",
    "def make_prediction(data, target_factor):\n",
    "    '''\n",
    "    Given the target variable, make a prediction.\n",
    "    data: pandas series for target variable\n",
    "    target_factor: boolean considering if the variable is a factor or not\n",
    "    '''\n",
    "\n",
    "    # Make predictions\n",
    "    if target_factor:\n",
    "        pred = data.value_counts().idxmax()\n",
    "    else:\n",
    "        pred = data.mean()\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b06673b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Weight <=  103': [{'Weight <=  74': [0,\n",
       "    {'Weight <=  84': [{'Weight <=  75': [1, 0]},\n",
       "      {'Weight <=  98': [1, 0]}]}]},\n",
       "  1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_tree(data,y, target_factor, max_depth = None,min_samples_split = None, \n",
    "               min_information_gain = 1e-20, counter=0, max_categories = 20):\n",
    "    '''\n",
    "    Trains a Decission Tree\n",
    "    data: Data to be used to train the Decission Tree\n",
    "    y: target variable column name\n",
    "    target_factor: boolean to consider if target variable is factor or numeric.\n",
    "    max_depth: maximum depth to stop splitting.\n",
    "    min_samples_split: minimum number of observations to make a split.\n",
    "    min_information_gain: minimum ig gain to consider a split to be valid.\n",
    "    max_categories: maximum number of different values accepted for categorical values. High number of values will \n",
    "    slow down learning process. R\n",
    "    '''\n",
    "\n",
    "    # Check that max_categories is fulfilled\n",
    "    if counter==0:\n",
    "        types = data.dtypes\n",
    "        check_columns = types[types == \"object\"].index\n",
    "        for column in check_columns:\n",
    "            var_length = len(data[column].value_counts()) \n",
    "            if var_length > max_categories:\n",
    "                raise ValueError('The variable ' + column + ' has '+ str(var_length) + \\\n",
    "                                 ' unique values, which is more than the accepted ones: ' +  str(max_categories))\n",
    "\n",
    "    # Check for depth conditions\n",
    "    if max_depth == None:\n",
    "        depth_cond = True\n",
    "\n",
    "    else:\n",
    "        if counter < max_depth:\n",
    "            depth_cond = True\n",
    "\n",
    "        else:\n",
    "            depth_cond = False\n",
    "\n",
    "    # Check for sample conditions\n",
    "    if min_samples_split == None:\n",
    "        sample_cond = True\n",
    "\n",
    "    else:\n",
    "        if data.shape[0] > min_samples_split:\n",
    "            sample_cond = True\n",
    "\n",
    "        else:\n",
    "            sample_cond = False\n",
    "\n",
    "    # Check for ig condition\n",
    "    if depth_cond & sample_cond:\n",
    "\n",
    "        var,val,ig,var_type = get_best_split(y, data)\n",
    "\n",
    "        # If ig condition is fulfilled, make split \n",
    "        if ig is not None and ig >= min_information_gain:\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            left,right = make_split(var, val, data,var_type)\n",
    "\n",
    "            # Instantiate sub-tree\n",
    "            split_type = \"<=\" if var_type else \"in\"\n",
    "            question =   \"{} {}  {}\".format(var,split_type,val)\n",
    "            # question = \"\\n\" + counter*\" \" + \"|->\" + var + \" \" + split_type + \" \" + str(val) \n",
    "            subtree = {question: []}\n",
    "\n",
    "            # Find answers (recursion)\n",
    "            yes_answer = train_tree(left,y, target_factor, max_depth,min_samples_split,min_information_gain, counter)\n",
    "            no_answer = train_tree(right,y, target_factor, max_depth,min_samples_split,min_information_gain, counter)\n",
    "\n",
    "            if yes_answer == no_answer:\n",
    "                subtree = yes_answer\n",
    "\n",
    "            else:\n",
    "                subtree[question].append(yes_answer)\n",
    "                subtree[question].append(no_answer)\n",
    "\n",
    "        # If it doesn't match IG condition, make prediction\n",
    "        else:\n",
    "            pred = make_prediction(data[y],target_factor)\n",
    "            return pred\n",
    "\n",
    "    # Drop dataset if doesn't match depth or sample conditions\n",
    "    else:\n",
    "        pred = make_prediction(data[y],target_factor)\n",
    "        return pred\n",
    "\n",
    "    return subtree\n",
    "\n",
    "max_depth = 5\n",
    "min_samples_split = 20\n",
    "min_information_gain  = 1e-5\n",
    "\n",
    "\n",
    "decisiones = train_tree(data,'obese',True, max_depth, min_samples_split, min_information_gain)\n",
    "\n",
    "\n",
    "decisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c10ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificar_datos(observacion, arbol):\n",
    "    question = list(arbol.keys())[0] \n",
    "\n",
    "    if question.split()[1] == '<=':\n",
    "        if observacion[question.split()[0]] <= float(question.split()[2]):\n",
    "            answer = arbol[question][0]\n",
    "        else:\n",
    "            answer = arbol[question][1]\n",
    "\n",
    "    else:\n",
    "        if observacion[question.split()[0]] in (question.split()[2]):\n",
    "            answer = arbol[question][0]\n",
    "        else:\n",
    "            answer = arbol[question][1]\n",
    "\n",
    "    # If the answer is not a dictionary\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    else:\n",
    "        residual_tree = answer\n",
    "    \n",
    "    return clasificar_datos(observacion, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d4e9d77",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlgorithm Accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43maccuracy\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "print('Algorithm Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76811c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed03da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceff3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028a7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14c1a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34552c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

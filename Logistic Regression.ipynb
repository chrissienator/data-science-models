{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43efe489",
   "metadata": {},
   "source": [
    "# Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "295197a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    LogisticRegression fits a regression model with k regressors, to minimize the cross entropy/\n",
    "    log loss error between the observed target variable in the dataset, and the target predicted \n",
    "    by the approximation.\n",
    "    \n",
    "    Needed packages:\n",
    "    import numpy as np\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fit_intercept : bool, default = True,\n",
    "        Specifies if a constant should be added to the regression.\n",
    "    \n",
    "    batch_size : int, default = 1000\n",
    "        Size of the batch in SGD loop.\n",
    "    \n",
    "    no_epochs : int, default = 10000\n",
    "        Number of epochs taken for the SGD to converge.\n",
    "        \n",
    "    learning_rate : float, default = 0.01\n",
    "        Learning rate in SGD.\n",
    "    \n",
    "    tolerance : float, default = 1e-10\n",
    "        Tolerance for stopping criteria.\n",
    "    \n",
    "    verbose : bool, default = False\n",
    "        Boolean to print or suppress epochs in SGD loop.\n",
    "        \n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    n_features : int\n",
    "        Number of features seen during method `fit`, excluding intercept.\n",
    "        \n",
    "    classes_ : ndarray of shape n_classes\n",
    "        A list of class labels seen during method `fit`.\n",
    "    \n",
    "    n_classes : int\n",
    "        Number of target classes seen during method `fit`.\n",
    "    \n",
    "    coefficients : ndarray of shape n_features + 1\n",
    "        Fitted coefficient of the features in the decision function. When fit_intercept is\n",
    "        set to true, coefficients includes bias term, hence first value is intercept.\n",
    "\n",
    "    intercept : ndarray of shape 1\n",
    "        Fitted intercept, if added to the decision function. First value in coefficients.\n",
    "        Set to 0.0 if fit_intercept = False.\n",
    "        \n",
    "    residuals : array of shape N\n",
    "        Estimated residuals, defined as the difference between the predicted,\n",
    "        and the true y-value.\n",
    "        \n",
    "    deviance_residuals : array of shape N\n",
    "        Estimated deviance residuals.\n",
    "\n",
    "    \n",
    "    Methods\n",
    "    ----------\n",
    "    fit(X,y):\n",
    "        Fit Logistic regression model.\n",
    "        \n",
    "    predict(X, boundary=0.5):\n",
    "        Predict using the fitted parameters of this Logistic Regression estimator.\n",
    "        \n",
    "    predict_prob(X, boundary=0.5):\n",
    "        Predict probability using the fitted parameters of this Logistic Regression estimator.\n",
    "        \n",
    "    accuracy(y, y_pred)\n",
    "        Print accuracy of fitted Logistic Regression estimator\n",
    "    \n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[-1, 1], [1, 2], [3, 4], [-3, 1], [-1, 0]])\n",
    "    >>> y = np.array([1, 0, 1, 0, 0])\n",
    "\n",
    "    >>> regfit = LogisticRegression(verbose=False, no_epochs=100000, tolerance=1e-10, fit_intercept=True).fit(X,y)\n",
    "    >>> regfit.n_features\n",
    "    2\n",
    "    >>> regfit.classes\n",
    "    array([0, 1])\n",
    "    >>> regfit.n_classes\n",
    "    2\n",
    "    >>> regfit.coefficients\n",
    "    array([[-1.93422523],\n",
    "           [ 0.04175192],\n",
    "           [ 0.93863707]])\n",
    "    >>> regfit.intercept\n",
    "    array([-1.93422523])\n",
    "    >>> regfit.residuals\n",
    "    array([[-0.73833641],\n",
    "           [ 0.49620032],\n",
    "           [-0.12503628],\n",
    "           [ 0.24585473],\n",
    "           [ 0.1217484 ]])\n",
    "    >>> regfit.deviance_residuals\n",
    "    array([[-1.63749542],\n",
    "           [ 1.17096248],\n",
    "           [-0.5168614 ],\n",
    "           [ 0.75122601],\n",
    "           [ 0.50955307]])\n",
    "    >>> regfit.predict(X)\n",
    "    array([0, 0, 1, 0, 0])\n",
    "    >>> regfit.predict_prob(X)\n",
    "    array([[0.73833645, 0.26166355],\n",
    "           [0.50379972, 0.49620028],\n",
    "           [0.12503625, 0.87496375],\n",
    "           [0.75414522, 0.24585478],\n",
    "           [0.87825167, 0.12174833]])\n",
    "    >>> regfit.accuracy(y, regfit.y_pred)\n",
    "    0.8\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        fit_intercept = True,\n",
    "        batch_size = 1000, \n",
    "        no_epochs = 10000, \n",
    "        learning_rate = 0.01,\n",
    "        tolerance = 1e-10,\n",
    "        verbose = False,\n",
    "    ):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.batch_size = batch_size\n",
    "        self.no_epochs = no_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "            return 1/(1 + np.exp(-z))\n",
    "        \n",
    "    @staticmethod    \n",
    "    def log_loss(y, y_hat):\n",
    "            return -np.mean(y*(np.log(y_hat)) + (1-y)*np.log(1-y_hat))\n",
    "        \n",
    "    @staticmethod        \n",
    "    def gradients(X, y, y_hat):\n",
    "            return (1/len(y))* X.T@(y_hat - y)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit Logistic regression model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape Nxk (Training data)\n",
    "        y : array of shape N (Target values)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted Estimator.\n",
    "        \"\"\"\n",
    "        # Define number of features and classes\n",
    "        self.n_features = len(X[0])\n",
    "        self.classes = np.unique(y)\n",
    "        self.n_classes = len(self.classes)\n",
    "        \n",
    "        if self.fit_intercept == True:\n",
    "            X = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "\n",
    "        # N, k: sample size, number of features\n",
    "        N, k = X.shape\n",
    "\n",
    "        # Initialize weights\n",
    "        self.coefficients = np.zeros((k,1))\n",
    "\n",
    "        # Reshape y\n",
    "        y = y.reshape(N,1)\n",
    "\n",
    "        # Empty list to store losses\n",
    "        self.log_losses = []\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(self.no_epochs):\n",
    "            if self.verbose == True:\n",
    "                print(f'Running epoch: {epoch}/{self.no_epochs}')\n",
    "            for i in range((N-1)//self.batch_size + 1):\n",
    "\n",
    "                # Define batches, SGD\n",
    "                start_i = i*self.batch_size\n",
    "                end_i = start_i + self.batch_size\n",
    "                xb = X[start_i:end_i]\n",
    "                yb = y[start_i:end_i]\n",
    "\n",
    "                # Calculate hypothesis/prediction.\n",
    "                y_hat = self.sigmoid(xb@self.coefficients)\n",
    "\n",
    "                # Get the gradients of loss w.r.t parameters.\n",
    "                grad = self.gradients(xb, yb, y_hat)\n",
    "\n",
    "                # Update the parameters.\n",
    "                self.coefficients -= self.learning_rate*grad\n",
    "\n",
    "            # Find previous log_loss\n",
    "            try:\n",
    "                l_prev = l.copy()\n",
    "            except:\n",
    "                l_prev = np.nan\n",
    "\n",
    "            # Calculate log_loss\n",
    "            l = self.log_loss(y, self.sigmoid(X@self.coefficients))\n",
    "    \n",
    "            # Stop SGD if tolerance is low\n",
    "            if l_prev - l < self.tolerance:\n",
    "                break\n",
    "                \n",
    "            self.log_losses.append(l)\n",
    "            \n",
    "        if self.fit_intercept == True:\n",
    "            self.intercept = self.coefficients[0]\n",
    "        else:\n",
    "            self.intercept = 0.0 \n",
    "            \n",
    "        # Define residuals and deviance residuals\n",
    "        self.residuals = y_hat - y\n",
    "        self.deviance_residuals = np.sign(self.residuals) * np.sqrt(-2*(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))\n",
    "        \n",
    "        return self\n",
    "        \n",
    "       \n",
    "    def predict(self, X, boundary=0.5):\n",
    "        \"\"\"\n",
    "        Predict target using the fitted parameters of this Logistic Regression estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape M, k-1\n",
    "            Values for regressors test data. (Don't add 1 for the intercept)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array of shape M\n",
    "            Returns predicted values.\n",
    "        \"\"\"\n",
    "        \n",
    "        if hasattr(self, 'coefficients') == False:\n",
    "                raise ValueError(\n",
    "                     \" This LogisticRegression instance is not fitted yet.\" \\\n",
    "                     \" Call 'fit' method with appropriate X and y before using this predict function.\"\n",
    "                 )\n",
    "        \n",
    "        if self.fit_intercept == True:\n",
    "            X = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "        \n",
    "        # Calculate predictions/y_hat.\n",
    "        self.y_hat = self.sigmoid(X@self.coefficients)\n",
    "\n",
    "        # If y_pred >= boundary: predict 1, else 0 and store predictions.\n",
    "        self.y_pred = np.array([1 if i > boundary else 0 for i in self.y_hat])\n",
    "        \n",
    "        return self.y_pred\n",
    "    \n",
    "    def predict_prob(self, X, boundary=0.5):\n",
    "        \"\"\"\n",
    "        Predict probability using the fitted parameters of this Logistic Regression estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape M, k-1\n",
    "            Values for regressors test data. (Don't add 1 for the intercept)\n",
    "            \n",
    "        boundary : float\n",
    "            Value of decision boundary. If estimated probability > boundary, predict 1.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array of shape M\n",
    "            Returns predicted values.\n",
    "        \"\"\"\n",
    "        \n",
    "        if hasattr(self, 'coefficients') == False:\n",
    "                raise ValueError(\n",
    "                     \" This LogisticRegression instance is not fitted yet.\" \\\n",
    "                     \" Call 'fit' method with appropriate X and y before using this predict function.\"\n",
    "                 )\n",
    "        \n",
    "        self.predict(X)\n",
    "        \n",
    "        probs = np.append(1-self.y_hat, self.y_hat, axis=1)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def accuracy(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Print accuracy of fitted Logistic Regression estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array of shape N\n",
    "            Values for true test data.\n",
    "        y_pred : array of shape N\n",
    "            Values for predicted targets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : constant\n",
    "            Returns accuracy.\n",
    "        \"\"\"\n",
    "        \n",
    "        if hasattr(self, 'coefficients') == False:\n",
    "                raise ValueError(\n",
    "                     \" This LogisticRegression instance is not fitted yet.\" \\\n",
    "                     \" Call 'fit' method with appropriate X and y before using this predict function.\"\n",
    "                 )\n",
    "                \n",
    "        accuracy = sum(y==y_pred) / len(y)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3e035",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf6b80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ng/83h0md1s73567d977zch9wnh0000gn/T/ipykernel_16601/944353457.py:221: RuntimeWarning: invalid value encountered in sqrt\n",
      "  self.deviance_residuals = np.sign(self.residuals) * np.sqrt(-2*(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, 1], [1, 2], [3, 4], [-3, 1], [-1, 0]])\n",
    "y = np.array([1, 0, 4, 0, 0])\n",
    "\n",
    "regfit = LogisticRegression(verbose=False, no_epochs=100000, tolerance=1e-10, fit_intercept=True).fit(X,y)\n",
    "regfit.n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffed9fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regfit.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e86b696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regfit.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c6daaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.04308925],\n",
       "       [4.21019069],\n",
       "       [5.25354458]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regfit.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ffbbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04308925])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regfit.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc847ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.11257793e-01],\n",
       "       [ 9.99999848e-01],\n",
       "       [-3.00000000e+00],\n",
       "       [ 1.81349561e-03],\n",
       "       [ 4.08802670e-02]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regfit.residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6102e311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4856914 ],\n",
       "       [ 5.60355676],\n",
       "       [        nan],\n",
       "       [ 0.06025184],\n",
       "       [ 0.28892684]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regfit.deviance_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3db597c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regfit.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff6d3a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73833645, 0.26166355],\n",
       "       [0.50379972, 0.49620028],\n",
       "       [0.12503625, 0.87496375],\n",
       "       [0.75414522, 0.24585478],\n",
       "       [0.87825167, 0.12174833]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regfit.predict_prob(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d61785e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regfit.accuracy(y, regfit.y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade6c4bf",
   "metadata": {},
   "source": [
    "# Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b996f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    OLS Linear Regression.\n",
    "\n",
    "    LinearRegression fits a linear model with k regressors, with fitted coefficients \n",
    "    b = (b1, ..., bk) to minimize the residual sum of squares between the observed \n",
    "    target variable in the dataset, and the target predicted by the linear approximation.\n",
    "    \n",
    "    Needed packages:\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fit_intercept : bool, default=True\n",
    "        Whether to calculate the intercept for this model. If set to False, \n",
    "        no intercept will be used in calculations (e.g. use if data is centered).\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    coefficients : array of shape k+1: #features/regressors + 1\n",
    "        Estimated coefficients for the linear regression problem.\n",
    "        This includes the intercept, as first value in the array.\n",
    "        \n",
    "    intercept : array of shape 1\n",
    "    Independent term/constant in the linear model. Set to 0.0 if fit_intercept = False.\n",
    "    \n",
    "    residuals : array of shape N\n",
    "        Estimated residuals, defined as the difference between the predicted,\n",
    "        and the true y-value\n",
    "        \n",
    "    se_coefficients : array of shape k+1\n",
    "        Estimated standard errors of estimated regression coefficients.\n",
    "    \n",
    "    t_values : array of shape k+1\n",
    "        Estimated t-values of estimated regression coefficients, for H0: b=0.\n",
    "    \n",
    "    p_values : array of shape k+1\n",
    "        Estimated p-values of estimated regression coefficients, for H0: b=0.\n",
    "\n",
    "    n_features : int\n",
    "        Number of features seen during method `fit`, excluding intercept.\n",
    "        \n",
    "\n",
    "        \n",
    "    Methods\n",
    "    ----------\n",
    "    fit(X, y): \n",
    "        Fit linear model, and create attributes\n",
    "    \n",
    "    predict(X): \n",
    "        Predict target variable, using the fitted parameters of this estimator.\n",
    "    \n",
    "    R_squared(X,y): \n",
    "        Return the coefficient of determination of the prediction.\n",
    "    \n",
    "    adjusted_R_squared(X,y): \n",
    "        Return the adjusted coefficient of determination of the prediction.\n",
    "    \n",
    "    summary(decimals): \n",
    "        Print summary of regression output after fit.\n",
    "    \n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "    >>> y = np.array([[0], [4], [0], [6]])\n",
    "    >>> regfit = LinearRegression(fit_intercept=True).fit(X, y)\n",
    "    >>> regfit.summary()\n",
    "    Regression output:\n",
    "               Coefficient  S.E.       t-value    p-value   \n",
    "    Intercept  -1.5         1.658      -0.905     0.532     \n",
    "    X1         -4.0         1.414      -2.828     0.216     \n",
    "    X2         5.0          1.0        5.0        0.126     \n",
    "\n",
    "    Residuals:\n",
    "    [-0.5  0.5  0.5 -0.5]\n",
    "\n",
    "    R-squared: 0.963               \t Adjusted R-squared: 0.889\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        fit_intercept=True,\n",
    "    ):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit linear regression model, simple OLS.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape Nxk (Training data)\n",
    "        y : array of shape N (Target values)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted Estimator.\n",
    "        \"\"\"\n",
    "        # Define n_features and n_observations\n",
    "        self.n_observations = len(y)\n",
    "        self.n_features = len(X[0])\n",
    "        \n",
    "        # Compute mean and sd of dependent variable\n",
    "        self.ymean = y.mean()\n",
    "        self.ystd = y.std()\n",
    "        \n",
    "        # Compute coefficients\n",
    "        if self.fit_intercept == True:\n",
    "            X = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "            self.coefficients = (np.linalg.inv(X.T@X)@np.transpose(X)@y).T\n",
    "            self.intercept = self.coefficients[0]\n",
    "        else:\n",
    "            self.coefficients = (np.linalg.inv(X.T@X)@np.transpose(X)@y).T\n",
    "            self.intercept = 0.0\n",
    "            \n",
    "        # Compute residuals\n",
    "        self.y_predicted = self.coefficients @ X.T\n",
    "        self.residuals = y - self.y_predicted.T\n",
    "        \n",
    "        # Compute standard error of coefficients and of regression\n",
    "        var_hat = (self.residuals.T @ self.residuals) / (len(y) - self.n_features - 1)\n",
    "        self.se_coefficients = np.sqrt(np.diag(var_hat * np.linalg.inv(X.T@X)))\n",
    "        self.se_regression = np.sqrt(var_hat)\n",
    "        \n",
    "        # Compute t- and p-values for Null hypothesis H0: b=0\n",
    "        self.t_values = self.coefficients / self.se_coefficients\n",
    "        if self.fit_intercept == True:\n",
    "            df = len(X) - len(X[0])\n",
    "        else:\n",
    "            df = len(X) - len(X[0]) - 1\n",
    "        self.p_values = 2*(stats.t.sf(abs(self.t_values), df))\n",
    "        \n",
    "        # Compute 95% CI\n",
    "        self.lowerCI = self.coefficients - stats.t.isf(0.025, self.n_observations-self.n_features-1) * self.se_coefficients\n",
    "        self.upperCI = self.coefficients + stats.t.isf(0.025, self.n_observations-self.n_features-1) * self.se_coefficients\n",
    "        \n",
    "        # Compute SSR, SST, SSE\n",
    "        self.SST = np.sum((y-y.mean())**2)\n",
    "        self.SSR = np.sum(self.residuals**2)\n",
    "        self.SSE = self.SST - self.SSR\n",
    "        \n",
    "        # Compute llf\n",
    "        self.llf = -np.log(self.SSR)*(self.n_observations/2) - (1+np.log(np.pi/(self.n_observations/2)))*(self.n_observations / 2)\n",
    "        \n",
    "        # Compute selection criteria RMSE, MAE, AIC, BIC, HQIC\n",
    "        self.RMSE = np.sqrt(np.mean((y - self.y_predicted)**2))\n",
    "        self.MAE = np.mean(abs(y - self.y_predicted))\n",
    "        self.AIC = -2 * self.llf + 2 * (self.n_features + 1)\n",
    "        self.BIC = -2 * self.llf + np.log(self.n_observations) * (self.n_features + 1)\n",
    "        self.HQIC = -2 * self.llf + 2 * (self.n_features + 1) * np.log(np.log(self.n_observations))\n",
    "        \n",
    "        # Compute F-statistic\n",
    "        self.F_statistic = (self.SSE/(self.n_features))/ (self.SSR/(self.n_observations - self.n_features - 1))\n",
    "        self.F_statistic_p_value = stats.f.sf(self.F_statistic, self.n_features, self.n_observations-self.n_features-1)\n",
    "        \n",
    "        # Compute residual skewness, kurtosis\n",
    "        self.res_skewness = np.mean(self.residuals**3) / (np.mean(self.residuals**2)**1.5)\n",
    "        self.res_kurtosis = np.mean(self.residuals**4) / (np.mean(self.residuals**2)**2)\n",
    "        \n",
    "        # Compute Durbin-Watson test statistic\n",
    "        self.diff_residuals = np.diff(self.residuals, axis=0)\n",
    "        self.DW_statistic = np.sum(self.diff_residuals**2) / np.sum(self.residuals**2)\n",
    "        \n",
    "        # Compute JB test statistic and p-value\n",
    "        self.JB_statistic = len(self.residuals) / 6 * (self.res_skewness**2 + (self.res_kurtosis - 3)**2 / 4)\n",
    "        self.JB_statistic_p_value = stats.chi2.sf(self.JB_statistic, 2)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using the fitted parameters of this Linear Regression estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape M, k-1\n",
    "            M values for k-1 regressors test data. Don't add 1 for the intercept.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array of shape M\n",
    "            Returns predicted value.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'coefficients') == False:\n",
    "                raise ValueError(\n",
    "                     \" This LinearRegression instance is not fitted yet.\" \\\n",
    "                     \" Call 'fit' method with appropriate X and y before using this predict function.\"\n",
    "                 )\n",
    "                \n",
    "        if self.fit_intercept == True:\n",
    "            X = np.vstack([np.ones(len(X)), X.T]).T\n",
    "        \n",
    "        y_predicted = self.coefficients @ X.T\n",
    "\n",
    "        return y_predicted.T\n",
    "    \n",
    "    def R_squared(self, X, y):\n",
    "        \"\"\"Return the coefficient of determination of the prediction.\n",
    "\n",
    "        The coefficient of determination R^2 is defined as the residual\n",
    "        sum of squares divided by the total sum of squares. The best possible R^2\n",
    "        score is 1.0, explaining all variance in the data. The R^2 score can be negative\n",
    "        when no constant is included, or the model is made arbitrarily bad. A model \n",
    "        always predicting the mean of y, the expected value of y, disregarding the input \n",
    "        features, gets a R^2 score of 0.0.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape Nxk\n",
    "        y : array of shape N\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        R_squared : float\n",
    "            R^2 score of predicted X wrt. true y\n",
    "        \"\"\"\n",
    "        \n",
    "        if hasattr(self, 'coefficients') == False:\n",
    "                raise ValueError(\n",
    "                     \" This LinearRegression instance is not fitted yet.\" \\\n",
    "                     \" Call 'fit' method with appropriate X and y before using this score function.\"\n",
    "                 )\n",
    "        \n",
    "        SSR = sum((y - self.predict(X))**2)\n",
    "        SST = sum((y - y.mean())**2)\n",
    "        R_squared = (1 - SSR/SST)\n",
    "        \n",
    "        return R_squared\n",
    "    \n",
    "    def adjusted_R_squared(self, X, y):\n",
    "        \"\"\"Return the adjusted coefficient of determination of the prediction.\n",
    "\n",
    "        The adjusted coefficient of determination R^2 is defined as the residual\n",
    "        sum of squares divided by N-k, divided by the total sum of squares, divided by N-1. \n",
    "        This allows penalty for #regressors.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of shape Nxk\n",
    "        y : array of shape N\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        adjusted_R_squared : float\n",
    "            Adjusted R^2 score of predicted X wrt. true y\n",
    "        \"\"\"\n",
    "        \n",
    "        if hasattr(self, 'coefficients') == False:\n",
    "                raise ValueError(\n",
    "                     \" This LinearRegression instance is not fitted yet.\" \\\n",
    "                     \" Call 'fit' method with appropriate X and y before using this score function.\"\n",
    "                 )\n",
    "        \n",
    "        N = len(X)\n",
    "        k = len(X[0])\n",
    "        \n",
    "        R2 = self.R_squared(X,y)\n",
    "        adjusted_R_squared = 1 - ((N-1)/(N-k-1))* (1 - R2)\n",
    "        \n",
    "        return adjusted_R_squared\n",
    "    \n",
    "    \n",
    "    def summary(self, decimals=3):\n",
    "        \"\"\"Returns the table of the regression output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        decimals : int, default=3\n",
    "            Number of decimals to round to in the table.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        regression_table : table\n",
    "            Table of the regression output.\n",
    "        \"\"\"\n",
    "        \n",
    "        if hasattr(self, 'coefficients') == False:\n",
    "                raise ValueError(\n",
    "                     \" This LinearRegression instance is not fitted yet.\" \\\n",
    "                     \" Call 'fit' method with appropriate X and y before using this predict function.\"\n",
    "                 )\n",
    "\n",
    "        print('Regression output:')\n",
    "        table = np.vstack([self.coefficients, \n",
    "                        self.se_coefficients,\n",
    "                        self.t_values, \n",
    "                        self.p_values]).T\n",
    "\n",
    "        regression_dict = dict()\n",
    "        for num in range(len(self.coefficients)):\n",
    "            if num == 0:\n",
    "                if self.fit_intercept == True:\n",
    "                    regression_dict['Intercept'] = table[num]\n",
    "                else:\n",
    "                    regression_dict['X1'] = table[num]\n",
    "            else:\n",
    "                if self.fit_intercept == True:\n",
    "                    string = f\"X{num}\"\n",
    "                    regression_dict[string] = table[num]\n",
    "                else:\n",
    "                    string = f\"X{num+1}\"\n",
    "                    regression_dict[string] = table[num]\n",
    "        \n",
    "        if decimals < 4:\n",
    "            L = 10\n",
    "        else:\n",
    "            L = decimals + 7\n",
    "            \n",
    "                \n",
    "        print(\"{:<{L}} {:<{L2}} {:<{L}} {:<{L}} {:<{L}}\".format('', 'Coefficient', 'S.E.', 't-value', 'p-value', \n",
    "                                                                L=L, L2=L+2))\n",
    "        \n",
    "        for k, v in regression_dict.items():\n",
    "            estim, se, tval, pval = v\n",
    "            print(\"{:<{L}} {:<{L2}} {:<{L}} {:<{L}} {:<{L}}\".format(k, \n",
    "                                                              np.round(estim, decimals), \n",
    "                                                              np.round(se, decimals), \n",
    "                                                              np.round(tval, decimals),\n",
    "                                                              np.round(pval,decimals), L=L, L2=L+2))\n",
    "\n",
    "        print('\\n(First 5) Residuals:')\n",
    "        print(self.residuals[:5])\n",
    "\n",
    "        print(f'\\nR-squared: {np.round(self.R_squared(X,y), decimals)} \\\n",
    "              \\t Adjusted R-squared: {np.round(self.adjusted_R_squared(X,y), decimals)}')\n",
    "        \n",
    "        \n",
    "    def html_summary(self):\n",
    "        \"\"\"Returns HTML table of the regression output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        regression_table : table\n",
    "            Table of the regression output.\n",
    "        \"\"\"\n",
    "        \n",
    "        dt = datetime.now()\n",
    "        \n",
    "        variable_names = []\n",
    "        for num in range(len(self.coefficients)):\n",
    "            if num == 0:\n",
    "                if self.fit_intercept == True:\n",
    "                    variable_names.append('Intercept')\n",
    "                else:\n",
    "                    variable_names.append('X1')\n",
    "            else:\n",
    "                if self.fit_intercept == True:\n",
    "                    string = f\"X{num}\"\n",
    "                else:\n",
    "                    string = f\"X{num+1}\"\n",
    "                variable_names.append(string)\n",
    "                    \n",
    "        table = np.vstack([[\"Variable\", \"Coefficient\", \"Std. error\", \"t-stat\", \"p-value\"], \n",
    "                           np.vstack([variable_names,\n",
    "                                      self.coefficients, \n",
    "                                      self.se_coefficients,\n",
    "                                      self.t_values, \n",
    "                                      self.p_values]).T])\n",
    "        \n",
    "        regression_table = tabulate(table, headers=\"firstrow\", tablefmt='html')\n",
    "        \n",
    "        text = HTML(f\"\"\"\n",
    "        <h2>Linear Regression Results</h2>\n",
    "\n",
    "        <p> </p>\n",
    "        <pre> Dependent variable: y </pre>\n",
    "        <pre> Method: Least squares </pre>\n",
    "        <pre> Date: {dt.month}/{dt.day}/{dt.year}, Time: {dt.hour}:{dt.minute}</pre>\n",
    "        <pre> Observations: {self.n_observations} </pre>\n",
    "        <pre> Variables (excluding intercept): {self.n_features} </pre>\n",
    "        <pre> D.o.f. Residuals: {self.n_observations - self.n_features - 1}</pre>\n",
    "        <pre> D.o.f. Model: {self.n_features} </pre>\n",
    "\n",
    "        {regression_table}\n",
    "\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr><th>Criterions and statistics   </th></tr>\n",
    "            </thead>\n",
    "            <tr>\n",
    "                <td>R-squared</td>\n",
    "                <td>{self.R_squared(X,y):6.6f}</td>\n",
    "                <td></td>        \n",
    "                <td></td>\n",
    "                <td>Mean dependent var.</td>\n",
    "                <td>{self.ymean:6.6f}</td>\n",
    "                <td></td>        \n",
    "                <td></td>\n",
    "                <td>Durbin-Watson statistic</td>\n",
    "                <td>{self.DW_statistic:6.6f}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Adjusted R-squared</td>\n",
    "                <td>{self.adjusted_R_squared(X,y):6.6f}</td>\n",
    "                <td></td>        \n",
    "                <td></td>\n",
    "                <td>SD dependent var.</td>\n",
    "                <td>{self.ystd:6.6f}</td>\n",
    "                <td></td>        \n",
    "                <td></td>\n",
    "                <td>Jarque-Bera statistic</td>\n",
    "                <td>{self.JB_statistic:6.6f}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>S.E. of regression</td>\n",
    "                <td>{self.se_regression:6.6f}</td>\n",
    "                <td></td>        \n",
    "                <td></td>\n",
    "                <td>Akaike information crit.</td>\n",
    "                <td>{self.AIC:6.6f}</td>\n",
    "                <td></td>        \n",
    "                <td></td>\n",
    "                <td>Prob(Jarque-Bera)</td>\n",
    "                <td>{self.JB_statistic_p_value:6.6f}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Sum squared residuals</td>\n",
    "                <td>{self.SSR:6.6f}</td>\n",
    "                <td></td>        \n",
    "                <td></td>\n",
    "                <td>Bayesian information crit.</td>\n",
    "                <td>{self.BIC:6.6f}</td>\n",
    "                <td></td>        \n",
    "                <td></td>\n",
    "                <td>Skewness residuals</td>\n",
    "                <td>{self.res_skewness:6.6f}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Log Likelihood</td>\n",
    "                <td>{self.llf:6.6f}</td>\n",
    "                <td></td>        \n",
    "                <td></td>\n",
    "                <td>Hanann-Quin information crit.</td>\n",
    "                <td>{self.HQIC:6.6f}</td>\n",
    "                <td></td>        \n",
    "                <td></td>\n",
    "                <td>Kurtosis residuals</td>\n",
    "                <td>{self.res_kurtosis:6.6}</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>F-statistic</td>\n",
    "                <td>{self.F_statistic:6.6}</td>\n",
    "                <td></td>        \n",
    "                <td></td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Prob(F-statistic)</td>\n",
    "                <td>{self.F_statistic_p_value:6.6}</td>\n",
    "                <td></td>        \n",
    "                <td></td>\n",
    "            </tr>\n",
    "        </table>\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        display(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd9f7b",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071d970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML\n",
    "from datetime import datetime\n",
    "from tabulate import tabulate\n",
    "import sklearn.datasets\n",
    " \n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "X = data.values\n",
    "y = target.values\n",
    "\n",
    "regfit = LinearRegression(fit_intercept=False).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c51446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression output:\n",
      "           Coefficient  S.E.       t-value    p-value   \n",
      "X1         -0.084       0.049      -1.714     0.089     \n",
      "X2         -0.024       0.057      -0.413     0.68      \n",
      "X3         0.225        0.057      3.955      0.0       \n",
      "X4         0.6          0.094      6.37       0.0       \n",
      "\n",
      "(First 5) Residuals:\n",
      "[0.07861541 0.04993583 0.06023686 0.00445714 0.07252236]\n",
      "\n",
      "R-squared: 0.93               \t Adjusted R-squared: 0.928\n"
     ]
    }
   ],
   "source": [
    "regfit.summary(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12059e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <h2>Linear Regression Results</h2>\n",
       "\n",
       "        <p> </p>\n",
       "        <pre> Dependent variable: y </pre>\n",
       "        <pre> Method: Least squares </pre>\n",
       "        <pre> Date: 12/22/2022, Time: 20:44</pre>\n",
       "        <pre> Observations: 150 </pre>\n",
       "        <pre> Variables (excluding intercept): 4 </pre>\n",
       "        <pre> D.o.f. Residuals: 145</pre>\n",
       "        <pre> D.o.f. Model: 4 </pre>\n",
       "\n",
       "        <table>\n",
       "<thead>\n",
       "<tr><th>Variable  </th><th style=\"text-align: right;\">  Coefficient</th><th style=\"text-align: right;\">  Std. error</th><th style=\"text-align: right;\">   t-stat</th><th style=\"text-align: right;\">    p-value</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>X1        </td><td style=\"text-align: right;\">   -0.0844926</td><td style=\"text-align: right;\">   0.0492986</td><td style=\"text-align: right;\">-1.71389 </td><td style=\"text-align: right;\">0.088685   </td></tr>\n",
       "<tr><td>X2        </td><td style=\"text-align: right;\">   -0.0235621</td><td style=\"text-align: right;\">   0.0570272</td><td style=\"text-align: right;\">-0.413173</td><td style=\"text-align: right;\">0.68009    </td></tr>\n",
       "<tr><td>X3        </td><td style=\"text-align: right;\">    0.224871 </td><td style=\"text-align: right;\">   0.0568609</td><td style=\"text-align: right;\"> 3.95476 </td><td style=\"text-align: right;\">0.000119403</td></tr>\n",
       "<tr><td>X4        </td><td style=\"text-align: right;\">    0.599722 </td><td style=\"text-align: right;\">   0.0941437</td><td style=\"text-align: right;\"> 6.37029 </td><td style=\"text-align: right;\">2.35072e-09</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "\n",
       "        <table>\n",
       "            <thead>\n",
       "                <tr><th>Criterions and statistics   </th></tr>\n",
       "            </thead>\n",
       "            <tr>\n",
       "                <td>R-squared</td>\n",
       "                <td>0.929996</td>\n",
       "                <td></td>        \n",
       "                <td></td>\n",
       "                <td>Mean dependent var.</td>\n",
       "                <td>1.000000</td>\n",
       "                <td></td>        \n",
       "                <td></td>\n",
       "                <td>Durbin-Watson statistic</td>\n",
       "                <td>1.149262</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>Adjusted R-squared</td>\n",
       "                <td>0.928065</td>\n",
       "                <td></td>        \n",
       "                <td></td>\n",
       "                <td>SD dependent var.</td>\n",
       "                <td>0.816497</td>\n",
       "                <td></td>        \n",
       "                <td></td>\n",
       "                <td>Jarque-Bera statistic</td>\n",
       "                <td>0.110746</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>S.E. of regression</td>\n",
       "                <td>0.219724</td>\n",
       "                <td></td>        \n",
       "                <td></td>\n",
       "                <td>Akaike information crit.</td>\n",
       "                <td>-24.018674</td>\n",
       "                <td></td>        \n",
       "                <td></td>\n",
       "                <td>Prob(Jarque-Bera)</td>\n",
       "                <td>0.946132</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>Sum squared residuals</td>\n",
       "                <td>7.000398</td>\n",
       "                <td></td>        \n",
       "                <td></td>\n",
       "                <td>Bayesian information crit.</td>\n",
       "                <td>-8.965498</td>\n",
       "                <td></td>        \n",
       "                <td></td>\n",
       "                <td>Skewness residuals</td>\n",
       "                <td>-0.006112</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>Log Likelihood</td>\n",
       "                <td>17.009337</td>\n",
       "                <td></td>        \n",
       "                <td></td>\n",
       "                <td>Hanann-Quin information crit.</td>\n",
       "                <td>-17.903047</td>\n",
       "                <td></td>        \n",
       "                <td></td>\n",
       "                <td>Kurtosis residuals</td>\n",
       "                <td>3.13255</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>F-statistic</td>\n",
       "                <td>481.578</td>\n",
       "                <td></td>        \n",
       "                <td></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>Prob(F-statistic)</td>\n",
       "                <td>1.27824e-82</td>\n",
       "                <td></td>        \n",
       "                <td></td>\n",
       "            </tr>\n",
       "        </table>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regfit.html_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
